---
title: "Incremental datasets for learning curves"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(rsample)
source("R/learning_curve.R")
library(ggplot2)
```


it takes random values but never training samples for test nor the other way
around.


You're figuring out if your algorithm is actually working. 
evaluate bias viarnace of machine learning model 
One way to evaluate that is using learning curves. 
You train the model on a very small sample of data
predict on that same data (training error)
evaluate on another small sample (test error)
take a larger piece of data and train model
predict on that same data (training error)
predict on larger piece of test data (test error)
keep increasing the size of the data points
plot points and line for training error and test error on same graph


```{r}
iris2 <- incremental_set(iris,min_data_size = 10)
print(iris2)
```
As you can see, this returns a rsample object with splits.



```{r, unpacking the splits}
all_rows <- 1:nrow(iris)
result <- list()
for (i in 1:nrow(iris2)) {
    result[[i]] <-indice_per_row(iris2[i,])
}
result <- bind_rows(result)
```

plot the overlap

```{r}
result %>% 
    filter(label=="Increment09") %>% 
    ggplot(aes(ind,type, color=type))+
    geom_point()+
    geom_rug()+
    ggtitle("\'training\' and \'test\' samples never overlap")
```


```{r}
ggplot(result, aes(ind, label, color = type))+
    geom_point()+
    facet_grid(.~type)
```

Dataset becomes larger and larger:

```{r}
result %>% 
    group_by(label, type) %>% 
    summarize(N = n()) %>% 
    mutate(incr = as.integer(gsub('Increment', "",label))) %>% 
    ggplot(aes(incr, N, color = type))+
    geom_point()+
    geom_line(alpha =1/2)
```

